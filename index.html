<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shengkun Tang</title>
  
  <meta name="author" content="Shengkun Tang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-GX5X02K4TX"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-GX5X02K4TX');
</script>

<body>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shengkun (Bryson) Tang</name>
              </p>
              <p>Welcome to my website~ My name is Shengkun Tang. You can call me Bryson for short. 
                Currently, I am a first-year PhD student of Machine Learning in <a href="https://mbzuai.ac.ae/">MBZUAI</a>, under the supervision of <a href="https://zhiqiangshen.com/">Prof. Zhiqiang Shen</a>. 
                During my gap year, I had a wonderful time as an research assistant in <a href="https://github.com/IST-DASLab"> DASLab</a> in <a href="https://ista.ac.at/en/home/">ISTA </a>, working with <a href="https://daslab.ista.ac.at/">Prof. Dan Alistarh</a>.
                Besides, I had close collaboration with <a href="https://dongkuanx27.github.io/">Prof. Dongkuan Xu</a> (NCSU) and <a href="https://yaqingwang.github.io/">Dr. Yaqing Wang</a> (Google DeepMind), working on efficent multi-modal models.
                I finished B.E. in <a href="https://rsgis.whu.edu.cn/">Remote Sensing</a> at <a href="https://www.whu.edu.cn/">Wuhan University </a>, under the supervision of <a href="https://www.scholat.com/jianyao">Prof. Jian Yao</a> and <a href="http://jszy.whu.edu.cn/suxin1/en/index.htm">Prof. Xin Su</a>.
              </p>
              
              <p style="text-align:center">
                <a href="shengkuntangwork@gmail.com">Email</a> &nbsp/&nbsp
                <!-- <a href="data/Shengkun_Tang.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com.hk/citations?user=m7ZA6vIAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/CarrMichael92">Twitter</a> &nbsp/&nbsp -->
                <a href="https://github.com/Tangshengku/">Github</a>
              </p>
              <p style="text-align:right"> Last updated: Feb. 4th 2025</p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ShengkunTang_circle.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ShengkunTang_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>News</heading>
        <tr>
          <div class="list scroll">
            <p>
              08/2024: Start my PhD life in MBZUAI.
            </p>
            <p>
              05/2023: Invited to serve as Reviewer for <a href="https://ncsu-dk-lab.github.io/workshops/relkd@2023/"><b><font color="black">International Workshop on Resource-Efficient Learning for Knowledge Discovery </font></b></a> at KDD 2023. 
            </p>
            <p>
              05/2023: Invited to give a talk at <a href="http://www.thejiangmen.com/"><b><font color="black">Â∞ÜÈó®ÂàõÊäï</font></b></a> on June 8, 2023. Welcome! 
            </p>
            <p>
              02/2023: My first paper on <strong style="color:red">accelerating inference of vision language model</strong> was accept by CVPR 2023. This is my first work before PhD Program. Super excited :). Thank all co-authors' support.
            </p>
            <p>
              09/2022: I joined <strong>Intelligent Automotive Group(IAG)</strong> at SenseTime as a system developer. I will build system for various perception modules of self-driving.
            </p>
          </td>
        </div>
        </tr>
      </tbody></table>
        <br>
        <br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <heading>Research</heading>
          <tr>
              <p>
              My research interests lie on Landable Artificial Intelligence, focusing on the <strong style="color: red;">Resource Efficiency</strong> and <strong style="color: red;">Trustworthy</strong> of <strong style="color: blue;">AI System</strong>. My research covers the whole pipeline of AI system, providing full-stack solutions from theoretical optimization methods and data-centric strategies to the development of efficient, interpretable and reliable deep learning techniques and the co-design of algorithms and hardware. 
              </p>
              <ul>
                <li><p>
                  <strong>Resource-Efficient Training & Inference Algorithms</strong>
                </p>
                <li><p>
                  <strong>Data Optimization to Improve Data Quality & Efficiency</strong>
                </p>
                <li><p>
                  <strong>Scalable Methods for AI Systems with Theoretical Guarantees</strong>
                </p>
                <li><p>
                  <strong>Algorithm-Hardware Co-design for Acceleration</strong>
                </p>
                <li><p> 
                  <strong>Application Scenario: Multi-Modal (Vision-Language), Uni-Modal (NLP, Computer Vision)</strong>
                </p>
    
                </ul>
                <strong style="color: red;">If you are interested in my research and seeking for collaboration, feel free to contact me. Any kinds of collaboration are welcome.</strong>
            </td>
          </tr>
        </tbody></table>
        <br>
        <!-- Publications -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Publications</heading>
          <br>
          <br>
          <tr onmouseout="malle_stop()" onmouseover="malle_start()">
            <td style="padding:20px;width:25%;vertical-align:middle;">
                <div class="one">
                  <br>
                  <br>
                    <img src='images/DALD.png' width="160" ></div>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle;">
                <papertitle>DALD: Improving Logits-based Detector without Logits from Black-box LLM</papertitle>
              </a>
              <br>
              <a>Cong Zeng*</a>, 
              <strong>Shengkun Tang*</strong>,
              <a>Xianjun Yang</a>, 
              <a>Yuanzhou Chen</a>,
              <a>Yiyou Sun,</a>
              <a>Yao Li</a>,
              <a>Haifeng Chen</a>,
              <a>Wei Cheng</a>,
              <a>Dongkuan Xu</a>
            </br>
              <em><strong>[NeurIPS 2024]</strong></em> The Thirty-eighth Annual Conference on Neural Information Processing Systems
              <br>
              <a href="https://arxiv.org/abs/2406.05232">arXiv</a> /
              <a href="https://github.com/cong-zeng/DALD">code</a> 
              
              <p>
              We propose a simple but quite effective method to improve the performance of black-box LLM detection. DALD collects a small-size data from target model and train the surrogate model to align the distribution of surrogate model and target model.            </p>
              </td>
            </tr>
          <tr onmouseout="malle_stop()" onmouseover="malle_start()">
            <td style="padding:20px;width:25%;vertical-align:middle;">
                <div class="one">
                  <br>
                  <br>
                    <img src='images/adadiff.png' width="160" ></div>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle;">
                <papertitle>Adadiff: Accelerating diffusion models through step-wise adaptive computation</papertitle>
              </a>
              <br>
              <strong>Shengkun Tang</strong>,
              <a>Yaqing Wang</a>, 
              <a>Caiwen Ding</a>,
              <a>Yi Liang,</a>
              <a>Yao Li</a>,
              <a>Dongkuan Xu</a>
            </br>
              <em><strong>[ECCV 2024]</strong></em> European Conference on Computer Vision
              <br>
              <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10120.pdf">arXiv</a> /
              <a href="https://github.com/Tangshengku/AdaDiff">code</a> 
              <p>
                We propose a uncertainty estimation module (UEM) to decide the exiting point during diffusion model inference at each timestep. Moreover, we propose an uncertainty-aware layer-wise loss to recover the performance for early-exited model.              </p>
              </td>
            </tr>

          <tr onmouseout="malle_stop()" onmouseover="malle_start()">
          <td style="padding:20px;width:25%;vertical-align:middle;">
              <div class="one">
                <br>
                <br>
                  <img src='images/MuE_Arc.jpg' width="160" ></div>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle;">
              <papertitle>You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model</papertitle>
            </a>
            <br>
            <strong>Shengkun Tang</strong>,
            <a>Yaqing Wang</a>, 
            <a>Zhenglun Kong</a>,
            <a>Tianchi Zhang,</a>
            <a>Yao Li</a>,
            <a>Caiwen Ding</a>,
            <a>Yanzhi Wang</a>,
            <a>Yi Liang</a>,
            <a>Dongkuan Xu</a>
            <br>
            <em><strong>[CVPR 2023]</strong></em> The IEEE/CVF Conference on Computer Vision and Pattern Recognition
            <br>
            <a href="https://arxiv.org/abs/2211.11152">arXiv</a> /
            <a href="https://github.com/OFA-Sys/OFA/tree/feature/MuE">code</a> 

              We propose a novel early exiting strategy based on cascading input similarity with valid assumptions on saturation states in visual-language models, a pioneering exploration of extending early exiting selection to encoders and decoders of sequence-to-sequence architectures.              </p>
            </td>
          </tr>
          <tr onmouseout="malle_stop()" onmouseover="malle_start()">
            <td style="padding:20px;width:25%;vertical-align:middle;">
              <div class="one">
                <br>
                <br>
                  <img src='images/DDRNet_arc.png' width="160" ></div>
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle;">
                <papertitle>DDR-Net: Learning Multi-Stage Multi-View Stereo With Dynamic Depth Range</papertitle>
              </a>
              <br><a>Puyuan Yi*</a>,
              <strong>Shengkun Tang*</strong>, 
              <a>Jian Yao</a>
              <br>
              <em>Preprint</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2103.14275">arXiv</a> /
              <a href="https://github.com/Tangshengku/DDR-Net">code</a> 
              <p>
                We propose a Dynamic Depth Range Network (DDR-Net) to determine the depth range hypotheses dynamically by applying a range estimation module (REM) to learn the uncertainties of range hypotheses in the former stages.              </p>
            </td>
          </tr>
					
          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <br>
                <br>
                <img src='images/DSNet.png' width="160" >
              </div>
              <script type="text/javascript">
                function nerfsuper_start() {
                  document.getElementById('nerfsuper_image').style.opacity = "1";
                }

                function nerfsuper_stop() {
                  document.getElementById('nerfsuper_image').style.opacity = "0";
                }
                nerfsuper_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Scale-robust deep-supervision network for mapping building footprints from high-resolution remote sensing images</papertitle>
              </a>
              <br>
              <a>Haonan Guo</a>, 
              <a>Xin Su</a>, 
              <strong>Shengkun Tang</strong>,
              <a> Bo Du</a>,
              <a>Liangpei Zhang</a>
              <br>
              <em>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em>, 2021
              <br>
							<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9535260">PDF</a>  
              <p>We propose a novel deep-supervision convolutional neural network (denoted as DS-Net) for extracting building footprints from high-resolution remote sensing images.</p>
            </td>
          </tr>


        </tbody></table>
        <!-- Intern Experience -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Intern Experience</heading>  
          <!-- <strong><heading>Undergraduate</heading></strong> -->
          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <br>
                <br>
                <br>
                <img src='images/sensetime.png' width="110">
              </div>
              <script type="text/javascript">
                function nerfsuper_start() {
                  document.getElementById('nerfsuper_image').style.opacity = "1";
                }

                function nerfsuper_stop() {
                  document.getElementById('nerfsuper_image').style.opacity = "0";
                }
                nerfsuper_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>SenseTime Engineering, 06/2021 - 10/2021 </papertitle>
              <br>
              <br> 
              Vision Algorithm Intern Researcher 
              <br>          
              Project: <a href="https://www.sensetime.com/en/news-detail/51166774?categoryId=1072">SenseRobot Chess Robotic</a>, working with <a href="https://dblp.org/pid/160/6811.html">Ruodai Li</a>
              <br>
            </td>
          </tr>

        </tbody></table>  

        <!-- Work Experience -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Work Experience</heading>  
          <!-- <strong><heading>Undergraduate</heading></strong> -->
          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <br>
                <br>
                <br>
                <img src='images/sensetime.png' width="110">
              </div>
              <script type="text/javascript">
                function nerfsuper_start() {
                  document.getElementById('nerfsuper_image').style.opacity = "1";
                }

                function nerfsuper_stop() {
                  document.getElementById('nerfsuper_image').style.opacity = "0";
                }
                nerfsuper_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>SenseTime, Intelligent Automotive Group(IAG), 05/2022 - Now </papertitle>
              <br>
              <br> 
              System Developer
              <br>          
              Project: Large-Scale Self-Driving System Development
              <br>
            </td>
          </tr>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <br>
                <br>
                <br>
                  <img src='images/sensetime.png' width="110"></div>
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>SenseTime, Smart City Group(SCG), 05/2022 - 10/2022</papertitle>
              <br> 
              <br>
              System Developer
              <br>          
              Project: Perception and Fusion Develop for Guangqi Project
              <br>
              <p></p>
              <p>
                In this project, we seek to solve some issues of multi-view stereo(MVS) including edge and light changes.</p>
            </td>
          </tr> -->

        </tbody></table>

        <!-- Contest -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Contest</heading>  
          <!-- <strong><heading>Undergraduate</heading></strong> -->
          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <br>
                <img src='images/baidu.png' width="110">
              </div>
              <script type="text/javascript">
                function nerfsuper_start() {
                  document.getElementById('nerfsuper_image').style.opacity = "1";
                }

                function nerfsuper_stop() {
                  document.getElementById('nerfsuper_image').style.opacity = "0";
                }
                nerfsuper_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://aistudio.baidu.com/aistudio/competition/detail/39/0/introduction"><papertitle>
                Baidu Astar Developer Competition,</a>
              05/2020 - 10/2020 </papertitle>
              <br>
              <br> 
              Ranking: <strong>7/2305</strong> (teams)
              <br>
              <br>          
              The task of Baidu Astar 2020 is traffic signs and surveillance cameras detection
              and matching. I was in charge of detection task. I solved the problems of data
              imbalance by using my own data argumentation strategy and detect surveil-
              lance cameras more accurately. We got into the final and rank 7 out of 2305
              teams.
              <br>
            </td>
          </tr>

        </tbody></table>  

        <!-- Projects -->
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Projects</heading>
          <br>
          <br>
          <strong><heading>Undergraduate</heading></strong>
          <tr onmouseout="malle_stop()" onmouseover="malle_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <br>
                  <img src='images/MVS_horse.jpg' width="160"></div>
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2103.14275">
                <papertitle>Multi-View Stereo in Real Scenes</papertitle>
              </a>, 06/2020 - 03/2021
              <br>
              <br><a>Puyuan Yi*</a>,
              <strong>Shengkun Tang*</strong>, 
              <a href="https://www.scholat.com/jianyao">Jian Yao</a></br>
              
              <a href="https://arxiv.org/abs/2103.14275">arXiv</a> /
              <a href="https://github.com/Tangshengku/DDR-Net">code</a> 
              <p></p>
              <p>
                In this project, we seek to solve some issues of multi-view stereo(MVS) including edge and light changes.</p>
            </td>
          </tr>
					
          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <br>
                <img src='images/SAR.jpeg' width="160">
              </div>
              <script type="text/javascript">
                function nerfsuper_start() {
                  document.getElementById('nerfsuper_image').style.opacity = "1";
                }

                function nerfsuper_stop() {
                  document.getElementById('nerfsuper_image').style.opacity = "0";
                }
                nerfsuper_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Synthetic Aperture Radar(SAR) Landmark Detection via Deep Learning and Dataset Constrcution </papertitle>, 09/2019 - 09/2021
              <br>
              <br>
              <strong>Shengkun Tang</strong>,
              <a href="http://jszy.whu.edu.cn/suxin1/en/index.htm">Xin Su</a>, 
              Ziyi Zhang,
              Xue Han,
              Fengxing Yang,
              Hulan Ayi
              <br>
              <p>This project aims to detect landmarks in SAR image. The deep learning methods of optical images fail in SAR images. We seek to fill the gap between radar and optical images.  </p>
            </td>
          </tr>


        </tbody></table> -->
				<!-- Professional Services -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Professional Services</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td width="75%" valign="center">
              <li>
                <b>Program Committee Member:</b>
                <ul>
                  <li>ICML 2025</li>
                  <li>ICLR 2025</li>
                  <li>AISTATS 2025</li>
                <li>NeurIPS 2024</li>
                <li>KDD 2023, 2024</li>
                <li>AAAI 2023</li>
                </ul>
            </td>
          </tr>
        
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This template comes from <a href="https://github.com/jonbarron/jonbarron_website">source code</a>, thanks for his fantastic website templates.
	            	
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
